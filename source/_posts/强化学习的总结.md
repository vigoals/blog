---
title: 强化学习的总结
date: 2016-12-04 10:38:31
tags:
---

最近一段时间都在学习reinforcement learning，并且尝试实现了常用的reinforcement learning 算法。在实现的过程中遇到了不少的问题，走了不少的弯路，所以想把自己这段时间学到的关于reinforcement learning的知识总结一下。

## alewrap
首先是游戏环境。因为我用torch作为机器学习的开发库，所以atari的游戏环境我用的是Deepmind的alewrap。

alewrap的安装我们可以参考"Human_Level_Control_through_Deep_Reinforcement_Learning"的代码。这里面有一个叫"install_dependencies.sh"的文件，可以对它进行简单的修改来安装alewrap。它的代码会重新安装torch及其他依赖的库，对于已经装好torch的情况，可以用下面我改过的代码。

	#!/usr/bin/env bash

	sudo apt-get update
	sudo apt-get install -qqy build-essential
	sudo apt-get install -qqy gcc g++
	sudo apt-get install -qqy cmake
	sudo apt-get install -qqy curl
	sudo apt-get install -qqy libreadline-dev
	sudo apt-get install -qqy git-core
	sudo apt-get install -qqy libjpeg-dev
	sudo apt-get install -qqy libpng-dev
	sudo apt-get install -qqy ncurses-dev
	sudo apt-get install -qqy imagemagick
	sudo apt-get install -qqy unzip
	sudo apt-get update

	path_to_nvcc=$(which nvcc)
	if [ -x "$path_to_nvcc" ]
	then
	    cutorch=ok
	    cunn=ok
	fi

	# Install base packages:
	luarocks install cwrap
	luarocks install paths
	luarocks install torch
	luarocks install nn

	[ -n "$cutorch" ] && \
	(luarocks install cutorch)
	[ -n "$cunn" ] && \
	(luarocks install cunn)

	luarocks install luafilesystem
	luarocks install penlight
	luarocks install sys
	luarocks install xlua
	luarocks install image
	luarocks install env

	echo ""
	echo "=> Torch7 has been installed successfully"
	echo ""


	echo "Installing nngraph ... "
	luarocks install nngraph
	RET=$?; if [ $RET -ne 0 ]; then echo "Error. Exiting."; exit $RET; fi
	echo "nngraph installation completed"

	echo "Installing Xitari ... "
	cd /tmp
	rm -rf xitari
	git clone https://github.com/deepmind/xitari.git
	cd xitari
	luarocks make
	RET=$?; if [ $RET -ne 0 ]; then echo "Error. Exiting."; exit $RET; fi
	echo "Xitari installation completed"

	echo "Installing Alewrap ... "
	cd /tmp
	rm -rf alewrap
	git clone https://github.com/deepmind/alewrap.git
	cd alewrap
	luarocks make
	RET=$?; if [ $RET -ne 0 ]; then echo "Error. Exiting."; exit $RET; fi
	echo "Alewrap installation completed"

	echo
	echo "You can run experiments by executing: "
	echo
	echo "   ./run_cpu game_name"
	echo
	echo "            or   "
	echo
	echo "   ./run_gpu game_name"
	echo
	echo "For this you need to provide the rom files of the respective games (game_name.bin) in the roms/ directory"
	echo

直接运行这段代码，就能完成alewrap环境的安装，以及实现DQN算法所需的依赖库。

现在我们来看看怎样使用alewrap库。我们可以下载它的源码来看看：

	git clone https://github.com/deepmind/alewrap.git

这里我们主要使用到了GameEnvironment类来对游戏环境进行管理。打开"alewrap/alewrap/GameEnvironment.lua"。这个类提供了开始新的游戏，按特定操作步进游戏等功能。这个类在初始化的时候需要提供下面几个参数:
* game_path 保存atari游戏rom包的地址。
* env 游戏的名字。
* actrep 每个step重复执行同一操作的次数。
* random_starts 用于随机开始新游戏，随机执行一定数目的空操作。

完成了游戏环境的初始化就可以直接获取游戏的初始状态了及基本的游戏信息了:

	local screen, reward, terminal = game_env:getState()
	local game_actions = game_env:getActions()

接下来就可以反复利用step函数来玩游戏了:

	while true
		--用各种算法获取接下来游戏所要执行的操作
		local action_index = torch.random(#game_actions)

		if not terminal then
			screen, reward, terminal =
				game_env:step(game_actions[action_index], true)
		else
			screen, reward, terminal = game_env:nextRandomGame()
		end
	end

这里需要注意的是step函数的第二个参数"true"。当要进行学习的过程中，这个参数需要被设置为"true"，这样，在游戏中每次掉命都会触发terminal(部分游戏有效果)。否则要到所有的命都没有之后才会触发terminal。评估Agent时无需第二个参数。nextRandomGame回随机执行一定的步数的空操作，用于使游戏处于一个随机状态之中。这个步数最大值由初始化时的random_starts指定。

到这里alewarp的基本使用就介绍完了。

## memory buffer
DQN在进行学习的时候，是从memory buffer中随机抽取样例的。至于为什么要这么做，这里就不说了。来说下样例保存时的注意事项。主要需要注意的就是screen的保存。我们用游戏的画面来作为游戏的状态，所以需要将游戏的画面screen保存在memroy buffer中，以便于后面可以取出来进行学习。但是直接保存游戏的画面非常占用内存，而且DQN学习过程中要保存1000000个游戏画面，所以要对画面进行简单的压缩处理。在Deepmind的DQN代码中，它把游戏画面压缩为84*84的float数组。这里需要注意的是，alewarp返回的screen是一个4维的Tensor。我们先要去掉一维，然后转化为二维灰度图的Tensor，再做压缩。

	if screen:dim() > 3 then
		screen = screen[1]
	end

	screen = image.rgb2y(screen)
	screen = image.scale(screen, width, height, 'bilinear')

这个时候screen就变成了一个二维的float类型的Tensor。这个时候我们需要将screen保存入memory buffer。因为screen的类型为float，会占用很大的内存，在保存的时候我们还需要进一步压缩screen的大小。

	screen:clone():float():mul(255):byte()

在使用screen的时候再把它变回来:

	screen:clone():float():div(255)

用image.display查看变换后的图像，是能够清晰的看到游戏的关键元素的。剩下就是将screen拼接为状态，以及如何随机取出minibatch，这些就不细说了。

## Deep Q-Network Learning
要进行学习，首先要建立网络，在"Human_Level_Control_through_Deep_Reinforcement_Learning"中，在使用GPU的情况下，它使用了nn.SpatialConvolutionCUDA作为卷积层。这个层已经过期了，现在直接用nn.SpatialConvolution就行了。网络的结构就参考Human的代码就行了。要注意的是输入的形式。一般卷积层的输入分为'NHWC'和'NCHW'。N是minibatch的大小，HW是height和width，C是channel的数目，这里就是state的hist_len。Torch采用的是NCHW的格式。

然后就是policy，将历史记录保存入memory以及采样进行学习。学习使用下面的公式:

![dqnDelta](/img/dqnDelta.gif)

'\theta_{i-1}'表示使用的是target网络。使用target网络可以使学习更加稳定。具体学习用的参数可以参考下Human。

## Agent保存与评估
Agent的保存直接就是用torch.save函数就行了，载入使用torch.load函数。评估的时候注意step不要带training参数。对于评估时使用nextRandomGame和newGame之间有什么区别还是没搞太明白。

## 总结
我基本上是把Human的代码按照自己的方式重写了一遍，很多细节的处理，以及参数的配置，网络的结构都是参照它的。重写一遍的好处是让我可以更加深刻的明白DQN实现过程中一些重要的细节。其实DQN本身并不难，反而是数据的保存，预处理这些比较麻烦，容易出错。可能还有许多地方没有说到，但是这里主要就写我自己认为需要注意的地方，其他的东西可以看看Human的代码，或者github上别人的DQN代码以及博客。还有Dueling Network以及Double DQN这些都是在DQN上进行一些改动就行了，也就不详细说了。
